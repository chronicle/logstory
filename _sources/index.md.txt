# LogStory

LogStory is used to update timestamps in telemetry (i.e. logs) and then replay them into a [Google Security Operations (SecOps)](https://cloud.google.com/security/products/security-operations?hl=en) tenant. Each usecase tells an infosec story, a "Log Story".

## Usecases

The stories are organized as "usecases", which always contain events and may contain entities, reference lists, and/or Yara-L 2.0 Detection Rules. Each usecase includes a ReadMe to describe its use.

Only the RULES_SEARCH_WORKSHOP is included witht the PyPI package. Learning about and installing addition usecases is described in [usecases](./usecase_docs/ReadMe.md).

```{tip} It is strongly recommended to review each usecase before ingestion rather than importing them all at once.
```

## Installation

Logstory has a command line interface (CLI), written in Python, that is most easily installed from the Python Package Index (PyPI):

```bash
$ pip install logstory
```

The `logstory` CLI interface has subcommands, which take arguments like so:
```
logstory replay_usecase RULES_SEARCH_WORKSHOP
```

These are explained in depth later in this doc.

## Configuration

After the subcommand, Logstory uses Google's [Abseil](https://abseil.io/docs/python/quickstart.html) library for parameterization of the CLI (aka "flags"). Once installed, it is easiest to configure the CLI flags in an [Abseil flagfile](https://abseil.io/docs/python/guides/flags#a-note-about---flagfile) like this one:

```
logstory replay_usecase RULES_SEARCH_WORKSHOP \
--customer_id=abcdef12-3456-4abc-8def-123456abcdef
--credentials_path=$HOME/.ssh/malachite-abcd1234abcd_ing.json
--timestamp_delta=1d  # optional
```

### Customer ID

(Required) This is your tenant's UUID4, which can be found at:

https://${code}.backstory.chronicle.security/settings/profile

### Credentials Path

(Required)  The credentials provided use the [Google Security Operations Ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api). This is *NOT* the newer RESTful v1alpha Ingestion API (yet, but that is future work).

**Getting API authentication credentials**

"Your Google Security Operations representative will provide you with a Google Developer Service Account Credential to enable the API client to communicate with the API."[[reference](https://cloud.google.com/chronicle/docs/reference/ingestion-api#getting_api_authentication_credentials)]


### Timestamp BTS

(Optional, default=1d) Updating timestamps for security telemetry is tricky. The .log files in the usecases have timestamps in many formats and we need to update them all to be recent while simultaneously preserving the relative differences between them. For each usecase, LogStory determines the base timestamp "bts" for the first timestamp in the first logfile and all updates are relative to it.


The image below shows that original timestamps on 2023-06-23 (top two subplots) were updated to 2023-09-24, the relative differences between the three timestamps on the first line of the first log file before (top left) and the last line of the logfile (top right) are preserved both interline and intraline on the bottom two subplots. The usecase spans an interval of 5 minutes and 55 seconds both before and after updates.

![My Image Description](_images/bts_update.jpg)

### Timestamp Delta

When timestamp_delta is set to 0d (zero days), only year, month, and day are updated (to today) and the hours, minutes, seconds, and milliseconds are preserved. That hour may be in the future, so when timestamp_delta is set to 1d the year, month, and day are set to today minus 1 day and the hours, minutes, seconds, and milliseconds are preserved.

```{tip}
For best results, use a cron jobs to run the usecase daily at 12:01am with `--timestamp_delta=1d`.
```

You may also provide `Nh` for offsetting by the hour, which is mainly useful if you want to replay the same log file multiple times per day (and prevend deduplication). Likewise, `Nm` offsets by minutes. These can be combined. For example, on the day of writing (Dec 13, 2024)`--timestamp_delta=1d1h1m` changes an original timestamp from/to:
```
2021-12-01T13:37:42.123Z1
2024-12-12T12:36:42.123Z1
```

The hour and minute were each offset by -1 and the date is the date of execution -1.


## Flags and flag files

Assuming your flagfile is named config.cfg, you can use it to define all of the required flags and then invoke with:

```
logstory replay_usecase_logtype RULES_SEARCH_WORKSHOP POWERSHELL --flagfile=config.cfg
```

That updates timestamps and all uploads from a single logfile in a single usecase. The following updates timestamps and uploads only entities (rather than events) from and overrides the timestamp_delta in the flagfile (if it is specified):

```
logstory replay_usecase_logtype RULES_SEARCH_WORKSHOP POWERSHELL --flagfile=config.cfg --timestamp_delta=0d --entities
```

You can increase verbocity with by prepending the python log level:
```
PYTHONLOGLEVEL=DEBUG logstory replay_usecase RULES_SEARCH_WORKSHOP --flagfile=config.cfg --timestamp_delta=0d
```

For more usage, see `logstory --help`


## Usecases

Usecases are meant to be self-describing, so check out the metadata in each one.

```{tip} It is strongly recommended to review each usecase before ingestion rather than importing them all at once.
```

As shown in the [ReadMe for the Rules Search Workshop](https://storage.googleapis.com/logstory-usecases-20241216/RULES_SEARCH_WORKSHOP/RULES_SEARCH_WORKSHOP.md),


If your usecases were distributed via PyPI (rather than git clone), they will be installed in `<venv>/site-packages/logstory/usecases`

You can find the absolute path to that usecase dir with:
```
python -c 'import os; import logstory; print(os.path.split(logstory.__file__)[0])'
/usr/local/home/dandye/miniconda3/envs/venv/lib/python3.13/site-packages/logstory
```

### Adding more usecases

We've chosen to distribute only a small subset of the available usecases. Should you choose to add more, you should read the metadata and understand the purpose of each one before adding them.


For the PyPI installed package, simply curl the new usecase into the `<venv>/site-packages/logstory/usecases` directory.

For example, first review the ReadMe for the EDR Workshop usecase:
https://storage.googleapis.com/logstory-usecases-20241216/EDR_WORKSHOP/EDR_WORKSHOP.md

Then download the usecase into that dir. For example:

```
gsutil rsync -r \
gs://logstory-usecases-20241216/EDR_WORKSHOP \
~/miniconda3/envs/venv/lib/python3.13/site-packages/logstory/usecases/
```

```{note}
This is onerous, so I'm going to add somecommands to make it easier.
```

```
ToDo:
 * [ ] Upload subset of usecases to a public bucket
   * [ ] add a management command to download from bucket
   * [ ] update readme with how to add
 * [ ] Add metadata to usecases
   * [ ] They should be self documenting
```

## Contributing

Interested in contributing? Check out the contributing guidelines. Please note that this project is released with a Code of Conduct. By contributing to this project, you agree to abide by its terms.

## License

`logstory` was created by Google Cloud Security. It is licensed under the terms of the Apache License 2.0 license.



## Development and re-building for publication on PyPI

```
git clone git@gitlab.com:google-cloud-ce/googlers/dandye/logstory.git
# Edit, edit, edit...
make build
# ToDo: pub to PyPI command
```


# GCP Cloud Run functions

The `cloudfunctions/` subdirectory contains Terraform configuration for deploying
the project to GCP Cloud Run functions. This includes:
 * Functions for loading Entities and Events on 3 day and 24 hour schedules
 * Scheduler for the above
 * GCP Cloud Storage bucket for the usecases
 * ...

## Prerequisites

```{warning}
Do not use GCP CloudShell to run Terraform, use your local machine, a Cloudtop instance (for Googlers), or a Linux VM in GCP.*
```

### Chronicle (Malachite) Google Cloud project configuration

The ingestion API used by LogStory is the [Google Security Operations Ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api).

```{seealso}
Reference documentation for the [Google Security Operations Ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api)
```

```{note}
Your Google Security Operations representative will provide you with a Google Developer Service Account Credential to enable the API client to communicate with the API.
```

- A Service Account is required in the `malachite-gglxxxx` Google Cloud project for your Chronicle tenant. Create a new Service Account if needed.
  - The Service Account needs the `Malachite Ingestion Collector` role assigned to it in order to forward events to Chronicle.
  - If you want to use Logstory's sample rules with Chronicle, the service account also needs the `Backstory Rules Engine API User` role.

### Google Cloud project configuration

Authenticate to the [Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects) that is bound to your Chronicle tenant.

~~~bash
gcloud auth login
PROJECT_ID=your_project_id_here
gcloud config set project $PROJECT_ID
~~~

Create a Service Account for use with Terraform and grant it the appropriate permissions.

~~~bash
gcloud iam service-accounts create terraform --display-name="terraform" --description="terraform"
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/cloudfunctions.admin
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/cloudscheduler.admin
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/iam.securityAdmin
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/iam.serviceAccountCreator
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/iam.serviceAccountDeleter
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/iam.serviceAccountUser
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/secretmanager.admin
gcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount:terraform@$PROJECT_ID.iam.gserviceaccount.com --role  roles/storage.admin
~~~

Enable the Google APIs in your project that are required to run Logstory.

~~~bash
gcloud services enable cloudresourcemanager.googleapis.com
gcloud services enable secretmanager.googleapis.com
gcloud services enable iam.googleapis.com
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable run.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable cloudbuild.googleapis.com
~~~

In the Google Cloud console, create a key for the newly created `terraform` Service Account. You will update the Terraform configuration files to use this key in an upcoming step.

A working [Terraform installation](https://developer.hashicorp.com/terraform/tutorials/gcp-get-started/install-cli) is required.

## Configuration

- [Clone this repository](https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html#clone-a-repository).
- If you have your own use cases to replay, add them to the [usecases](./usecases/) folder. By default, Logstory will replay all of the use cases in [usecases](./usecases/).
    - Open the [cloudfunction-code](./cloudfunction-code/) folder and review the `usecases_events_logtype_map.yaml` and `usecases_entities_logtype_map.yaml` files under each folder. If you want to disable a use case, just flip the `enabled` flag to **0**.
    - If you have new use cases, add them to corresponding *.yaml* files under each folder depending on the replay frequency.
- [Create a GCS Bucket](https://console.cloud.google.com/storage/) to save the Terraform state such as `TLA-chronicle-logstory-replay-tfstate`.
- Open [`backend.tf`](./backend.tf) with your favorite text editor and change both `credentials` and `bucket` values to yours.
    - `credentials` should be set to the path to your Terraform Service Account key file.
    - The bucket must have been created previously as it is where you will be storing your Terraform's state files.
- Open `variables.tf` with your favorite text editor and change the default values to yours including the Chronicle Ingestion API key.
    - Note, the **numeric** Google Cloud project number is required for the `gcp_project_number` variable. You can execute the following `gcloud` command to find it.

    ```bash
    gcloud projects list
    me:~/chronicle-logstory$ gcloud projects list
    PROJECT_ID        NAME       PROJECT_NUMBER
    chronicle-123456  chronicle  101010101010
    ```

## Deployment

- Run `sh bash.sh` to generate the latest version of Cloud Function codes
- Run the following Terraform commands to initialize, plan, and apply the configuration.

~~~bash
# Verify that the Terraform state file has been uploaded to the GCS bucket after running 'terraform init'.
terraform init
terraform plan
terraform apply
~~~

Navigate to the Cloud Functions and Cloud Scheduler pages in the Google Cloud console and verify that the Logstory artifacts were created.

## Cloud Schedulers

Logstory deploys a number of different Cloud Schedulers. Some data is ingested every 24hours and others every 3 days. If you want to ingest data immediately, you can force a run of the Cloud Scheduler job manually. However, we suggest you to run the logs in a specific order, first run the entities and after couple of hours, run the events.

There are some sample rules in this repo that can be deployed into your Chronicle as well. See [rules](./rules/) folder. If you want to deploy the sample rules, just force run the job named `TLA-chronicle-rule-creator-XXX` from Cloud Schedulers.

## Destruction

- Run `terraform destroy`
- Delete the GCS Bucket created to store the Terraform state.
- Delete the rules created in your Chronicle SIEM using [Delete Rule API](https://cloud.google.com/chronicle/docs/reference/detection-engine-api#deleterule)

## Authors and acknowledgment
security-adoption-eng@google.com



## Contents

```{toctree}
---
maxdepth: 2
---
usecase_docs/ReadMe
usecase_docs/index
```
